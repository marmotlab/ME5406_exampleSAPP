{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C for Single-Agent Path Planning (SAPP)\n",
    "\n",
    "While training is taking place, statistics on agent performance are available from Tensorboard. To launch it use:\n",
    "\n",
    "`tensorboard --logdir train_sapp_vanilla`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be the thing, right?\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import os\n",
    "import pickle\n",
    "import imageio\n",
    "\n",
    "import sapp_gym\n",
    "from ACNet import ACNet\n",
    "\n",
    "dev_list = torch.cuda.is_available()\n",
    "print(dev_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(images, fname, duration=2, true_image=False, salience=False, salIMGS=None):\n",
    "    imageio.mimwrite(fname,images,subrectangles=True)\n",
    "    print(\"wrote gif\")\n",
    "\n",
    "\n",
    "def discount(x, gamma):\n",
    "    return signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, game, global_network, trainer, device):\n",
    "        self.env = game\n",
    "        self.global_network = global_network\n",
    "        self.trainer = trainer\n",
    "        self.device = device\n",
    "\n",
    "        self.nextGIF = episode_count  # For GIFs output\n",
    "\n",
    "    def train(self, rollout, gamma, bootstrap_value):\n",
    "        global episode_count\n",
    "\n",
    "        rollout_size = len(rollout)\n",
    "        # obs, goal, a, r, train_valid, rnn_state0[0], rnn_state0[1]]\n",
    "        # [128, 3, 11, 11]\n",
    "        observ = torch.stack([rollout[i][0] for i in range(rollout_size)]).squeeze(1)\n",
    "        # [128, 3]\n",
    "        goal_vec = torch.stack([rollout[i][1] for i in range(rollout_size)]).squeeze(1)\n",
    "        # [128, 1]\n",
    "        actions = torch.stack([rollout[i][2] for i in range(rollout_size)])\n",
    "        # [128, 1]\n",
    "        rewards = torch.stack([rollout[i][3] for i in range(rollout_size)])\n",
    "        # [128, 5]\n",
    "        valids = torch.stack([rollout[i][5] for i in range(rollout_size)])\n",
    "        # [128, 1, 128]\n",
    "        h_n = torch.stack([rollout[i][6] for i in range(rollout_size)]).squeeze(1).permute(1, 0, 2)\n",
    "        # [128, 1, 128]\n",
    "        h_c = torch.stack([rollout[i][7] for i in range(rollout_size)]).squeeze(1).permute(1, 0, 2)\n",
    "\n",
    "        # Forward view\n",
    "        policies, values, policies_sig, _ = self.global_network(observ, goal_vec, (h_n, h_c))\n",
    "\n",
    "        # Backward view\n",
    "        # Calculate target values for value loss\n",
    "        # [129, 1]\n",
    "        rewards_plus = torch.cat((rewards, bootstrap_value), 0).detach().cpu().numpy()\n",
    "        discounted_rewards = discount(rewards_plus, gamma)[:-1]\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards.copy()).to(self.device)\n",
    "\n",
    "        # Calculate advantage values for policy loss\n",
    "        # Here we take the rewards and values from the rollout, and use them to\n",
    "        # generate the advantage and discounted returns. (With bootstrapping)\n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "\n",
    "        # [129, 1]\n",
    "        value_plus = torch.cat((values, bootstrap_value), 0).detach().cpu().numpy()\n",
    "        advantages = rewards.cpu().numpy() + gamma * value_plus[1:] - value_plus[:-1]\n",
    "        advantages = discount(advantages, gamma)\n",
    "        advantages = torch.FloatTensor(advantages.copy()).to(self.device)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        responsible_outputs = torch.gather(policies, 1, actions)\n",
    "\n",
    "        v_l = 0.5 * torch.sum(torch.square(discounted_rewards.detach() - values))\n",
    "        e_l = - 0.01 * torch.sum(policies * torch.log(torch.clip(policies, 1e-10, 1.0)))\n",
    "        p_l = - torch.sum(torch.log(torch.clip(responsible_outputs, 1e-15, 1.0)) * advantages.detach())\n",
    "        valid_l = - 0.5 * torch.sum(torch.log(torch.clip(policies_sig, 1e-10, 1.0)) * valids + torch.log(torch.clip(1 - policies_sig, 1e-10, 1.0)) * (1 - valids))\n",
    "        loss = v_l + p_l - e_l + valid_l\n",
    "        self.trainer.zero_grad()\n",
    "        loss.backward()\n",
    "        g_n = torch.nn.utils.clip_grad_norm_(self.global_network.parameters(), 1000)\n",
    "        self.trainer.step()\n",
    "        return v_l, p_l, valid_l, e_l, g_n\n",
    "\n",
    "    def work(self, max_episode_length, gamma):\n",
    "        global episode_count, episode_rewards, episode_lengths, episode_mean_values, episode_invalid_ops\n",
    "        total_steps, i_buf = 0, 0\n",
    "\n",
    "        while True:\n",
    "            episode_buffer, episode_values = [], []\n",
    "            episode_reward = episode_step_count = episode_inv_count = 0\n",
    "            d = False\n",
    "            # Initial state from the environment\n",
    "            s, validActions = self.env.reset()\n",
    "            h = torch.zeros(1, 1, 128).to(self.device)\n",
    "            c = torch.zeros(1, 1, 128).to(self.device)\n",
    "            rnn_state0 = (h, c)\n",
    "            rnn_state = rnn_state0\n",
    "\n",
    "            saveGIF = False\n",
    "            if OUTPUT_GIFS and ((not TRAINING) or (episode_count >= self.nextGIF)):\n",
    "                saveGIF = True\n",
    "                self.nextGIF = episode_count + 256\n",
    "                GIF_episode = int(episode_count)\n",
    "                episode_frames = [self.env._render(mode='rgb_array', screen_height=900, screen_width=900)]\n",
    "\n",
    "            while True:  # Give me something!\n",
    "                # Take an action using probabilities from policy network output.\n",
    "                obs = torch.FloatTensor(s[0]).unsqueeze(0).to(self.device)\n",
    "                goal = torch.FloatTensor(s[1]).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    a_dist, v, _, rnn_state = self.global_network(obs, goal, rnn_state)\n",
    "                if not (torch.argmax(a_dist.flatten()) in validActions):\n",
    "                    episode_inv_count += 1\n",
    "\n",
    "                train_valid = np.zeros(a_size)\n",
    "                train_valid[validActions] = 1\n",
    "\n",
    "                valid_dist = a_dist[0, validActions]\n",
    "                valid_dist /= torch.sum(valid_dist)\n",
    "\n",
    "                if TRAINING:\n",
    "                    #                        if(not (np.argmax(a_dist.flatten()) in validActions)):\n",
    "                    #                            episode_inv_count += 1\n",
    "                    #                            a     = validActions[ np.random.choice(range(valid_dist.shape[1])) ]\n",
    "                    #                        else:\n",
    "                    a = validActions[torch.multinomial(valid_dist, 1)]\n",
    "                else:\n",
    "                    a = torch.argmax(valid_dist)\n",
    "                    if a not in validActions or not GREEDY:\n",
    "                        a = validActions[torch.multinomial(valid_dist, 1)]\n",
    "\n",
    "                s1, r, d, validActions = self.env.step(a)\n",
    "\n",
    "                if saveGIF:\n",
    "                    episode_frames.append(self.env._render(mode='rgb_array', screen_width=900, screen_height=900))\n",
    "\n",
    "                r = torch.FloatTensor([r]).to(self.device)\n",
    "                a = torch.tensor([a], dtype=torch.int64).to(self.device)\n",
    "                train_valid = torch.FloatTensor(train_valid).to(self.device)\n",
    "                episode_buffer.append([obs, goal, a, r, v[0, 0], train_valid, rnn_state0[0], rnn_state0[1]])\n",
    "                episode_values.append(v[0, 0].cpu().numpy())\n",
    "                episode_reward += r.cpu().numpy()\n",
    "                s = s1\n",
    "                total_steps += 1\n",
    "                episode_step_count += 1\n",
    "                rnn_state0 = rnn_state\n",
    "\n",
    "                if d == True:\n",
    "                    print('\\n{} Goodbye World. We did it!'.format(episode_step_count), end='\\n')\n",
    "\n",
    "                # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                # make an update step using that experience rollout.\n",
    "                if TRAINING and (len(episode_buffer) % EXPERIENCE_BUFFER_SIZE == 0 or d):\n",
    "                    # Since we don't know what the true final return is, we \"bootstrap\" from our current value estimation.\n",
    "                    if len(episode_buffer) >= EXPERIENCE_BUFFER_SIZE:\n",
    "                        episode_buffer_training = episode_buffer[-EXPERIENCE_BUFFER_SIZE:]\n",
    "                    else:\n",
    "                        episode_buffer_training = episode_buffer[:]\n",
    "\n",
    "                    if d:\n",
    "                        s1Value = torch.FloatTensor([[0]]).to(self.device)\n",
    "                    else:\n",
    "                        obs = torch.FloatTensor(s[0]).unsqueeze(0).to(self.device)\n",
    "                        goal = torch.FloatTensor(s[1]).unsqueeze(0).to(self.device)\n",
    "                        with torch.no_grad():\n",
    "                            _, s1Value, _, _ = self.global_network(obs, goal, rnn_state)\n",
    "\n",
    "                    v_l, p_l, valid_l, e_l, g_n = self.train(episode_buffer_training, gamma, s1Value)\n",
    "                    rnn_state0 = rnn_state\n",
    "\n",
    "                if episode_step_count >= max_episode_length or d:\n",
    "                    break\n",
    "\n",
    "            episode_lengths.append(episode_step_count)\n",
    "            episode_mean_values.append(np.nanmean(episode_values))\n",
    "            episode_invalid_ops.append(episode_inv_count)\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "            if not TRAINING:\n",
    "                episode_count += 1\n",
    "                print('({}) Thread 0: {} steps, {:.2f} reward ({} invalids).'.format(episode_count, episode_step_count,episode_reward, episode_inv_count))\n",
    "                GIF_episode = int(episode_count)\n",
    "            else:\n",
    "                episode_count += 1\n",
    "\n",
    "                if episode_count % SUMMARY_WINDOW == 0:\n",
    "                    if episode_count % 100 == 0:\n",
    "                        print('Saving model', end='\\n')\n",
    "                        checkpoint = {\"model\": self.global_network.state_dict(),\n",
    "                                      \"optimizer\": self.trainer.state_dict(),\n",
    "                                      \"episode\": episode_count}\n",
    "                        path_checkpoint = \"./\" + model_path + \"/checkpoint.pth\"\n",
    "                        torch.save(checkpoint, path_checkpoint)\n",
    "                        print('Saved model', end='\\n')\n",
    "                    mean_reward = np.nanmean(episode_rewards[-SUMMARY_WINDOW:])\n",
    "                    mean_length = np.nanmean(episode_lengths[-SUMMARY_WINDOW:])\n",
    "                    mean_value = np.nanmean(episode_mean_values[-SUMMARY_WINDOW:])\n",
    "                    mean_invalid = np.nanmean(episode_invalid_ops[-SUMMARY_WINDOW:])\n",
    "\n",
    "                    writer.add_scalar(tag='Perf/Reward', scalar_value=mean_reward, global_step=episode_count)\n",
    "                    writer.add_scalar(tag='Perf/Length', scalar_value=mean_length, global_step=episode_count)\n",
    "                    writer.add_scalar(tag='Perf/Value', scalar_value=mean_value, global_step=episode_count)\n",
    "                    writer.add_scalar(tag='Perf/Valid Rate', scalar_value=(mean_length - mean_invalid) / mean_length, global_step=episode_count)\n",
    "\n",
    "                    writer.add_scalar(tag='Losses/Value Loss', scalar_value=v_l, global_step=episode_count)\n",
    "                    writer.add_scalar(tag='Losses/Policy Loss', scalar_value=p_l, global_step=episode_count)\n",
    "                    writer.add_scalar(tag='Losses/Valid Loss', scalar_value=valid_l, global_step=episode_count)\n",
    "                    writer.add_scalar(tag='Losses/Grad Norm', scalar_value=g_n, global_step=episode_count)\n",
    "\n",
    "                    if printQ:\n",
    "                        print('{} Tensorboard updated'.format(episode_count), end='\\r')\n",
    "\n",
    "            if saveGIF:\n",
    "                # Dump episode frames for external gif generation (otherwise, makes the jupyter kernel crash)\n",
    "                time_per_step = 0.1\n",
    "                images = np.array(episode_frames)\n",
    "                if TRAINING:\n",
    "                    make_gif(images,'{}/episode_{:d}_{:d}_{:.1f}.gif'.format(gifs_path, GIF_episode, episode_step_count, float(episode_reward)))\n",
    "                else:\n",
    "                    make_gif(images, '{}/episode_{:d}_{:d}.gif'.format(gifs_path, GIF_episode, episode_step_count), duration=len(images) * time_per_step, true_image=True, salience=False)\n",
    "            if SAVE_EPISODE_BUFFER:\n",
    "                with open('gifs3D/episode_{}.dat'.format(GIF_episode), 'wb') as file:\n",
    "                    pickle.dump(episode_buffer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "max_episode_length     = 128\n",
    "episode_count          = 0\n",
    "EPISODE_START          = episode_count\n",
    "gamma                  = .95 # discount rate for advantage estimation and reward discounting\n",
    "#moved network parameters to ACNet.py\n",
    "EXPERIENCE_BUFFER_SIZE = 128\n",
    "GRID_SIZE              = 11 #the size of the FOV grid to apply to each agent\n",
    "ENVIRONMENT_SIZE       = (10,32)#the total size of the environment (length of one side)\n",
    "OBSTACLE_DENSITY       = (0,.3) #range of densities\n",
    "DIAG_MVMT              = False # Diagonal movements allowed?\n",
    "a_size                 = 5 + int(DIAG_MVMT)*4\n",
    "SUMMARY_WINDOW         = 10\n",
    "LR_Q                   = 8.e-5\n",
    "load_model             = False\n",
    "RESET_TRAINER          = False\n",
    "model_path             = 'model_sapp_vanilla'\n",
    "gifs_path              = 'gifs_sapp_vanilla'\n",
    "train_path             = 'train_sapp_vanilla'\n",
    "GLOBAL_NET_SCOPE       = 'global'\n",
    "\n",
    "# Simulation options\n",
    "FULL_HELP              = False\n",
    "OUTPUT_GIFS            = True\n",
    "SAVE_EPISODE_BUFFER    = False\n",
    "\n",
    "# Testing\n",
    "TRAINING               = True\n",
    "GREEDY                 = False\n",
    "NUM_EXPS               = 100\n",
    "MODEL_NUMBER           = 313000\n",
    "\n",
    "# Shared arrays for tensorboard\n",
    "episode_rewards        = []\n",
    "episode_lengths        = []\n",
    "episode_mean_values    = []\n",
    "episode_invalid_ops    = []\n",
    "rollouts               = None\n",
    "printQ                 = False # (for headless)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Hello World\")\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not TRAINING:\n",
    "    gifs_path += '_tests'\n",
    "    if SAVE_EPISODE_BUFFER and not os.path.exists('gifs3D'):\n",
    "        os.makedirs('gifs3D')\n",
    "\n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(gifs_path):\n",
    "    os.makedirs(gifs_path)\n",
    "\n",
    "# with torch.device(\"cpu\"): # uncomment to run on GPU, and comment next line\n",
    "device = torch.device('cuda')\n",
    "\n",
    "master_network = ACNet(a_size, NUM_CHANNEL=3, GRID_SIZE=GRID_SIZE).to(device) # Generate global network\n",
    "optimizer = optim.Adam(master_network.parameters(), lr=LR_Q)\n",
    "\n",
    "gameEnv = sapp_gym.SAPPEnv(DIAGONAL_MOVEMENT=DIAG_MVMT, SIZE=ENVIRONMENT_SIZE,\n",
    "                           observation_size=GRID_SIZE, PROB=OBSTACLE_DENSITY)\n",
    "writer = SummaryWriter(train_path)\n",
    "\n",
    "if load_model:\n",
    "    print('Loading Model...')\n",
    "    checkpoint = torch.load(model_path + '/checkpoint.pth')\n",
    "    master_network.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    curr_episode = checkpoint['episode']\n",
    "    print(\"episode_count set to \", curr_episode)\n",
    "    if RESET_TRAINER:\n",
    "        optimizer = optim.Adam(master_network.parameters(), lr=LR_Q)\n",
    "\n",
    "worker = Worker(gameEnv, master_network, optimizer, device)\n",
    "worker.work(max_episode_length, gamma)\n",
    "\n",
    "\n",
    "if not TRAINING:\n",
    "    print([np.mean(episode_lengths), np.sqrt(np.var(episode_lengths)), np.mean(np.asarray(episode_lengths < max_episode_length, dtype=float))])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
