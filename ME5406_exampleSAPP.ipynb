{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C for Single-Agent Path Planning (SAPP)\n",
    "\n",
    "While training is taking place, statistics on agent performance are available from Tensorboard. To launch it use:\n",
    "\n",
    "`tensorboard --logdir train_sapp_vanilla`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should be the thing, right?\n",
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.signal as signal\n",
    "import os\n",
    "import pickle\n",
    "import imageio\n",
    "\n",
    "import sapp_gym\n",
    "from ACNet import ACNet\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "dev_list = device_lib.list_local_devices()\n",
    "print(dev_list)\n",
    "#assert len(dev_list) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(images, fname, duration=2, true_image=False,salience=False,salIMGS=None):\n",
    "    imageio.mimwrite(fname,images,subrectangles=True)\n",
    "    print(\"wrote gif\")\n",
    "\n",
    "def discount(x, gamma):\n",
    "    return signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, game, global_network):\n",
    "        self.env = game\n",
    "        self.global_network = global_network\n",
    "\n",
    "        self.nextGIF = episode_count # For GIFs output\n",
    "\n",
    "    def train(self, rollout, sess, gamma, bootstrap_value, rnn_state0, imitation=False):\n",
    "        global episode_count\n",
    "\n",
    "        # [s[0],s[1],a,r,s1,d,v[0,0],train_valid]\n",
    "        rollout     = np.array(rollout)\n",
    "        observ      = rollout[:,0]\n",
    "        goal_vec    = rollout[:,1]\n",
    "        actions     = rollout[:,2]\n",
    "        rewards     = rollout[:,3]\n",
    "        values      = rollout[:,6]\n",
    "        valids      = rollout[:,7]\n",
    "\n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. (With bootstrapping)\n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        feed_dict = {\n",
    "            self.global_network.target_v:np.stack(discounted_rewards),\n",
    "            self.global_network.inputs:np.stack(observ),\n",
    "            self.global_network.goal_pos:np.stack(goal_vec),\n",
    "            self.global_network.actions:actions,\n",
    "            self.global_network.train_valid:np.stack(valids),\n",
    "            self.global_network.advantages:advantages,\n",
    "            self.global_network.state_in[0]:rnn_state0[0],\n",
    "            self.global_network.state_in[1]:rnn_state0[1]\n",
    "        }\n",
    "        \n",
    "        v_l,p_l,valid_l,e_l,g_n,v_n,_ = sess.run([self.global_network.value_loss,\n",
    "            self.global_network.policy_loss,\n",
    "            self.global_network.valid_loss,\n",
    "            self.global_network.entropy,\n",
    "            self.global_network.grad_norms,\n",
    "            self.global_network.var_norms,\n",
    "            self.global_network.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "\n",
    "        return v_l, p_l, valid_l, e_l, g_n, v_n\n",
    "\n",
    "    def shouldRun(self, coord, episode_count):\n",
    "        if TRAINING:\n",
    "            return (not coord.should_stop())\n",
    "        else:\n",
    "            return (episode_count < NUM_EXPS)\n",
    "\n",
    "    def work(self, max_episode_length, gamma, sess, coord, saver):\n",
    "        global episode_count, episode_rewards, episode_lengths, episode_mean_values, episode_invalid_ops\n",
    "        total_steps, i_buf = 0, 0\n",
    "\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while self.shouldRun(coord, episode_count):\n",
    "                episode_buffer, episode_values = [], []\n",
    "                episode_reward = episode_step_count = episode_inv_count = 0\n",
    "                d = False\n",
    "\n",
    "                # Initial state from the environment\n",
    "                s, validActions       = self.env.reset()\n",
    "                rnn_state             = self.global_network.state_init\n",
    "                rnn_state0            = rnn_state\n",
    "\n",
    "                saveGIF = False\n",
    "                if OUTPUT_GIFS and ((not TRAINING) or (episode_count >= self.nextGIF)):\n",
    "                    saveGIF = True\n",
    "                    self.nextGIF = episode_count + 256\n",
    "                    GIF_episode = int(episode_count)\n",
    "                    episode_frames = [ self.env._render(mode='rgb_array',screen_height=900,screen_width=900) ]\n",
    "                    \n",
    "                while True: # Give me something!\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state = sess.run([self.global_network.policy,\n",
    "                                                   self.global_network.value,\n",
    "                                                   self.global_network.state_out],\n",
    "                                         feed_dict={self.global_network.inputs:[s[0]],\n",
    "                                                    self.global_network.goal_pos:[s[1]],\n",
    "                                                    self.global_network.state_in[0]:rnn_state[0],\n",
    "                                                    self.global_network.state_in[1]:rnn_state[1]})\n",
    "\n",
    "                    if(not (np.argmax(a_dist.flatten()) in validActions)):\n",
    "                        episode_inv_count += 1\n",
    "\n",
    "                    train_valid = np.zeros(a_size)\n",
    "                    train_valid[validActions] = 1\n",
    "\n",
    "                    valid_dist = np.array([a_dist[0,validActions]])\n",
    "                    valid_dist /= np.sum(valid_dist)\n",
    "\n",
    "                    if TRAINING:\n",
    "#                        if(not (np.argmax(a_dist.flatten()) in validActions)):\n",
    "#                            episode_inv_count += 1\n",
    "#                            a     = validActions[ np.random.choice(range(valid_dist.shape[1])) ]\n",
    "#                        else:\n",
    "                        a     = validActions[ np.random.choice(range(valid_dist.shape[1]),p=valid_dist.ravel()) ]\n",
    "                    else:\n",
    "                        a         = np.argmax(a_dist.flatten())\n",
    "                        if a not in validActions or not GREEDY:\n",
    "                            a     = validActions[ np.random.choice(range(valid_dist.shape[1]),p=valid_dist.ravel()) ]\n",
    "\n",
    "                    s1, r, d, validActions = self.env.step(a)\n",
    "\n",
    "                    if saveGIF:\n",
    "                        episode_frames.append(self.env._render(mode='rgb_array',screen_width=900,screen_height=900))\n",
    "\n",
    "                    episode_buffer.append([s[0],s[1],a,r,s1,d,v[0,0],train_valid])\n",
    "                    episode_values.append(v[0,0])\n",
    "                    episode_reward += r\n",
    "                    s = s1\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                    if d == True:\n",
    "                        print('\\n{} Goodbye World. We did it!'.format(episode_step_count), end='\\n')\n",
    "\n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "                    if TRAINING and (len(episode_buffer) % EXPERIENCE_BUFFER_SIZE == 0 or d):\n",
    "                        # Since we don't know what the true final return is, we \"bootstrap\" from our current value estimation.\n",
    "                        if len(episode_buffer) >= EXPERIENCE_BUFFER_SIZE:\n",
    "                            episode_buffer_training = episode_buffer[-EXPERIENCE_BUFFER_SIZE:]\n",
    "                        else:\n",
    "                            episode_buffer_training = episode_buffer[:]\n",
    "\n",
    "                        if d:\n",
    "                            s1Value = 0\n",
    "                        else:\n",
    "                            s1Value = sess.run(self.global_network.value, \n",
    "                                 feed_dict={self.global_network.inputs:np.array([s[0]])\n",
    "                                            ,self.global_network.goal_pos:[s[1]]\n",
    "                                            ,self.global_network.state_in[0]:rnn_state[0]\n",
    "                                            ,self.global_network.state_in[1]:rnn_state[1]})[0,0]\n",
    "\n",
    "                        v_l, p_l, valid_l, e_l, g_n, v_n = self.train(episode_buffer_training,sess,gamma,s1Value,rnn_state0)\n",
    "                        rnn_state0                       = rnn_state\n",
    "\n",
    "                    if episode_step_count >= max_episode_length or d:\n",
    "                        break\n",
    "\n",
    "                episode_lengths.append(episode_step_count)\n",
    "                episode_mean_values.append(np.nanmean(episode_values))\n",
    "                episode_invalid_ops.append(episode_inv_count)\n",
    "                episode_rewards.append(episode_reward)\n",
    "\n",
    "                if not TRAINING:\n",
    "                    episode_count += 1\n",
    "                    print('({}) Thread 0: {} steps, {:.2f} reward ({} invalids).'.format(episode_count, episode_step_count, episode_reward, episode_inv_count))\n",
    "                    GIF_episode = int(episode_count)\n",
    "                else:\n",
    "                    episode_count += 1\n",
    "\n",
    "                    if episode_count % SUMMARY_WINDOW == 0:\n",
    "                        if episode_count % 100 == 0:\n",
    "                            print ('Saving Model', end='\\n')\n",
    "                            saver.save(sess, model_path+'/model-'+str(int(episode_count))+'.cptk')\n",
    "                            print ('Saved Model', end='\\n')\n",
    "                        mean_reward = np.nanmean(episode_rewards[-SUMMARY_WINDOW:])\n",
    "                        mean_length = np.nanmean(episode_lengths[-SUMMARY_WINDOW:])\n",
    "                        mean_value = np.nanmean(episode_mean_values[-SUMMARY_WINDOW:])\n",
    "                        mean_invalid = np.nanmean(episode_invalid_ops[-SUMMARY_WINDOW:])\n",
    "\n",
    "                        summary = tf.Summary()\n",
    "                        summary.value.add(tag='Perf/Reward', simple_value=mean_reward)\n",
    "                        summary.value.add(tag='Perf/Length', simple_value=mean_length)\n",
    "                        summary.value.add(tag='Perf/Valid Rate', simple_value=(mean_length-mean_invalid)/mean_length)\n",
    "\n",
    "                        summary.value.add(tag='Losses/Value Loss', simple_value=v_l)\n",
    "                        summary.value.add(tag='Losses/Policy Loss', simple_value=p_l)\n",
    "                        summary.value.add(tag='Losses/Valid Loss', simple_value=valid_l)\n",
    "                        summary.value.add(tag='Losses/Grad Norm', simple_value=g_n)\n",
    "                        summary.value.add(tag='Losses/Var Norm', simple_value=v_n)\n",
    "                        global_summary.add_summary(summary, int(episode_count))\n",
    "\n",
    "                        global_summary.flush()\n",
    "\n",
    "                        if printQ:\n",
    "                            print('{} Tensorboard updated'.format(episode_count), end='\\r')\n",
    "\n",
    "                if saveGIF:\n",
    "                    # Dump episode frames for external gif generation (otherwise, makes the jupyter kernel crash)\n",
    "                    time_per_step = 0.1\n",
    "                    images = np.array(episode_frames)\n",
    "                    if TRAINING:\n",
    "                        make_gif(images, '{}/episode_{:d}_{:d}_{:.1f}.gif'.format(gifs_path,GIF_episode,episode_step_count,episode_reward))\n",
    "                    else:\n",
    "                        make_gif(images, '{}/episode_{:d}_{:d}.gif'.format(gifs_path,GIF_episode,episode_step_count), duration=len(images)*time_per_step,true_image=True,salience=False)\n",
    "                if SAVE_EPISODE_BUFFER:\n",
    "                    with open('gifs3D/episode_{}.dat'.format(GIF_episode), 'wb') as file:\n",
    "                        pickle.dump(episode_buffer, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning parameters\n",
    "max_episode_length     = 128\n",
    "episode_count          = 0\n",
    "EPISODE_START          = episode_count\n",
    "gamma                  = .95 # discount rate for advantage estimation and reward discounting\n",
    "#moved network parameters to ACNet.py\n",
    "EXPERIENCE_BUFFER_SIZE = 128\n",
    "GRID_SIZE              = 11 #the size of the FOV grid to apply to each agent\n",
    "ENVIRONMENT_SIZE       = (10,32)#the total size of the environment (length of one side)\n",
    "OBSTACLE_DENSITY       = (0,.3) #range of densities\n",
    "DIAG_MVMT              = False # Diagonal movements allowed?\n",
    "a_size                 = 5 + int(DIAG_MVMT)*4\n",
    "SUMMARY_WINDOW         = 10\n",
    "LR_Q                   = 8.e-5\n",
    "load_model             = False\n",
    "RESET_TRAINER          = False\n",
    "model_path             = 'model_sapp_vanilla'\n",
    "gifs_path              = 'gifs_sapp_vanilla'\n",
    "train_path             = 'train_sapp_vanilla'\n",
    "GLOBAL_NET_SCOPE       = 'global'\n",
    "\n",
    "# Simulation options\n",
    "FULL_HELP              = False\n",
    "OUTPUT_GIFS            = True\n",
    "SAVE_EPISODE_BUFFER    = False\n",
    "\n",
    "# Testing\n",
    "TRAINING               = True\n",
    "GREEDY                 = False\n",
    "NUM_EXPS               = 100\n",
    "MODEL_NUMBER           = 140500\n",
    "\n",
    "# Shared arrays for tensorboard\n",
    "episode_rewards        = []\n",
    "episode_lengths        = []\n",
    "episode_mean_values    = []\n",
    "episode_invalid_ops    = []\n",
    "rollouts               = None\n",
    "printQ                 = False # (for headless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "print(\"Hello World\")\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "if not TRAINING:\n",
    "    gifs_path += '_tests'\n",
    "    if SAVE_EPISODE_BUFFER and not os.path.exists('gifs3D'):\n",
    "        os.makedirs('gifs3D')\n",
    "\n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(gifs_path):\n",
    "    os.makedirs(gifs_path)\n",
    "\n",
    "#with tf.device(\"/gpu:0\"): # uncomment to run on GPU, and comment next line\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    trainer = tf.contrib.opt.NadamOptimizer(learning_rate=LR_Q, use_locking=True)\n",
    "\n",
    "    master_network = ACNet(GLOBAL_NET_SCOPE, a_size, trainer, True, GRID_SIZE) # Generate global network\n",
    "\n",
    "    gameEnv = sapp_gym.SAPPEnv(DIAGONAL_MOVEMENT=DIAG_MVMT, SIZE=ENVIRONMENT_SIZE, \n",
    "                               observation_size=GRID_SIZE, PROB=OBSTACLE_DENSITY)\n",
    "\n",
    "    worker  = Worker(gameEnv, master_network)\n",
    "\n",
    "    global_summary = tf.summary.FileWriter(train_path)\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "\n",
    "        if load_model == True:\n",
    "            print ('Loading Model...')\n",
    "            if not TRAINING:\n",
    "                with open(model_path+'/checkpoint', 'w') as file:\n",
    "                    file.write('model_checkpoint_path: \"model-{}.cptk\"'.format(MODEL_NUMBER))\n",
    "                    file.close()\n",
    "            ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "            p=ckpt.model_checkpoint_path\n",
    "            p=p[p.find('-')+1:]\n",
    "            p=p[:p.find('.')]\n",
    "            if TRAINING:\n",
    "                episode_count = int(p)\n",
    "            else:\n",
    "                episode_count = 0\n",
    "            saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "            print(\"episode_count set to \",episode_count)\n",
    "            if RESET_TRAINER:\n",
    "                trainer = tf.contrib.opt.NadamOptimizer(learning_rate=lr, use_locking=True)\n",
    "\n",
    "        worker.work(max_episode_length, gamma, sess, coord,saver)\n",
    "\n",
    "if not TRAINING:\n",
    "    print([np.mean(episode_lengths), np.sqrt(np.var(episode_lengths)), np.mean(np.asarray(np.asarray(episode_lengths) < max_episode_length, dtype=float))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
